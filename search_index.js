var documenterSearchIndex = {"docs":
[{"location":"content/intro/#Why-Should-I-Care-about-Graphics?","page":"Introduction","title":"Why Should I Care about Graphics?","text":"","category":"section"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"During my PhD, I got this question a lot. To be honest, it's a good question. If you are a scientist that studies the motion of galaxies (or some similar problem), why would you care about the latest animation from PIXAR or DreamWorks? Well, let's talk about that.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"As a reminder, GPU stands for Graphics Processing Unit. Historically, its purpose has been to do graphics. Games. Visualizations. You know. Graphics.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"These workflows typically require a lot of simple operations. For example, we might need to move a bunch of points from one set of locations to another. Or color a bunch pixels red (or any other color). Or to track a bunch of rays of light bouncing around a scene.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"It's not particularly difficult to whip up some code in Python, C, or Julia to solve these problems for us. The trouble comes from the fact that these operations often need to be done a lot – thousands or millions of times. We also usually need the results immediately – like within one sixtieth of a second. When there are a large number of operations and a really short time limit, it suddenly makes sense to offload computation to a separate device that is built for that kind of work.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"That's what the GPU is. A separate device that is built to solve a lot of simple operations at the same time.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"I need to stop and expand upon the three separate claims made in the previous statement.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"The GPU is a separate device: This means that we often need a special protocol to use it from our programming language of choice, and we need to think about how to transfer data to and from the GPU.\nThe GPU ... is built to solve ... simple operations: This means that certain workflows are not well-suited for the GPU. We'll give more examples of these later in the book.\nThe GPU ... is built to solve a lot of ... operations at the same time: This means that we need to actively think about what each computational core of the GPU is doing in parallel.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"I have often said that research in computational science mirrors research in computer graphics. Computer graphics researchers generally work on hardware and software tooling for GPUs – small, parallel devices that can fit on modern motherboards. Computational scientists generally work on hardware and software tooling for supercomputers – large, parallel networks of computers strung together to solve difficult problems. In a sense, both groups have been attempting to do the same thing: break up complex tasks into simpler ones so they make better use of parallel hardware. Eventually, the two forces met and General Purpose GPU (GPGPU) computing was born. Nowadays, the fastest supercomputers in the world use GPUs for computation. It's pretty clear that the GPU does more than \"just graphics.\"","category":"page"},{"location":"content/intro/#But-What-If-I-Actually-Care-About-Graphics?","page":"Introduction","title":"But What If I Actually Care About Graphics?","text":"","category":"section"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Another great question!","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"This book is specifically written for students who want to use their GPU for more general applications (like large-scale simulations). It is a little unfortunate that the programming interfaces used for graphics are typically quite different than those used for computing. If you are interested in building a game or rendering engine, it might be best to think of this book as a way to satiate some idle curiosity that might be lingering in the back of your head. It's all good to know, but it's ok to read it for fun instead of rigor.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"That said, there are still a number of good reasons to keep reading:","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"It is entirely possible to use the lessons learned from this book to do \"software rendering,\" which can be more flexible than traditional graphics workflows.\nWe'll be discussing several graphical applications that are well-suited for compute workflows, such as ray marching and splatting.\nEven within traditional graphics workflows, there are several applications that use \"compute shaders\" for various reasons (volume rendering and particle systems both come to mind). Compute shaders are almost identical to the functions we will be writing in this book.\nThis book should give you some key intuition about how and why the GPU works the way it does, which could be quite valuable for performance engineering down the road.\nWe will discuss the abstractions used in graphical GPU interfaces in the next chapter.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"But there is a larger question here. Why is there such a big difference between interfaces for graphics and interfaces for computation? After all, we are all programming for the same device, right? At the end of the day, it's all GPU.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Well, this brings something I really have to say.  An unfortunate truth about GPU computing in 2024 that all students must be aware of before proceeding further. No matter what language, interface, or method you decide to use to program for your GPU, they all share one thing in common: jank.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Simply put, GPU interfaces are way, way less polished than you might expect when transitioning from \"traditional\" CPU programming. Some of this is because GPUs are inherently parallel devices, while CPU code is often written without parallelism in mind. But I would argue that majority of programmers struggling with GPU programming in 2024 are not necessarily struggling with concepts, but are instead limited by the software used to implement those concepts.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"I think now is a good time to talk about the GPU ecosystem as a whole, and in particular...","category":"page"},{"location":"content/intro/#The-Big-Green-Elephant-in-the-Room","page":"Introduction","title":"The Big Green Elephant in the Room","text":"","category":"section"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Every now and again, I'll get stuck on a problem. It happens to the best of us, and I am certainly not one of the best. There are many different strategies to getting out of such a rut. Some people might take a walk and think about something else for a while. Others might drill down and try to find another solution from a different angle. Still others, might rip off all their clothes, jump in the bath, and have a deep, insightful conversation about their problem with a small yellow duck floating next to their head.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"I guess I usually take that last approach. Except I am fully clothed. And not usually submerged in a body of water. And my \"yellow duck\" is actually a bunch of random people who listen to me ramble about useless things while livestreaming on Twitch or YouTube.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"While streaming, I would often get asked questions. Some were useful. Some were not. But some questions were repeated time and time again. One such question was, \"As a beginner, how do I start programming?\"","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"It's a really good question without a clear answer. I would often say, \"the only way to start programming is just to start programming,\" and then follow up that statement with specific projects the person could work on to get them started. I find that working through a few meaningful examples is always a good way to start learning anything and have structured this book around that theme.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"But there's something deeply wrong with my statement. If someone is truly starting with absolutely no knowledge about programming, how do they even know where to start? What programming language do they use? What development environment? What concepts should they target first?","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"These are all good questions, and (in fact), might be the very same questions you are asking yourself right now when it comes to GPU programming. As hard as these problems are to answer, for most people, there are a few good starting points. You can't go wrong with Python or Julia as a starter language. If you want more rigor, go C or C++. If you want an active community, go Rust. Game devs might consider C#. Web devs, honestly, should talk to someone else. There are many such recommendations I could make based on the student's specific goals.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"But when it comes to GPU computing, there are no clear-cut guidelines. In fact, in 2024, there is no single language that I can truly recommend. For those who know GPU computing, you might be raising your eyebrow at the previous sentence. After all, there certainly is a single programming interface that has dominated the GPGPU space for literal decades. It has so much market share, that the company in charge of its design is now one of the most profitable companies in the history of our planet. Yes, I am talking about NVIDIA and their programming interface CUDA.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Yet, the fastest supercomputers in the world today (in 2024), do not use cards from NVIDIA and are largely unable to run CUDA code. If you are a research scientist targeting these devices, it might be a good idea to choose another language.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"And what about \"real\" graphics? What if you want to make a game or animation? Well, you will probably be using OpenGL, DirectX, or Vulkan. More accurately, you will probably be using some sort of game or graphics engine built on top of any of the aforementioned tools. Regardless, it's pretty clear you will not be using CUDA.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"If you want your code to run on a smartphone GPU, CUDA is not suitable. If you want the code to run performantly on a parallel CPU configuration without having to rewrite everything, yet again, CUDA is not suitable. If you have an AMD, Intel, or Apple Silicon GPU, yet again (again), CUDA is not suitable.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Now, please keep in mind, I am not discouraging the use of CUDA. It is a fantastic language that has been the unofficial king / queen of GPU computing for a long time and there are a lot of good reasons for that. In fact, I am actively encouraging you to rewrite all the code in this book in CUDA if you want.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"I am mainly just echoing a point I made before. The current state of GPU computing is messy. Even though most programmers go with CUDA by default, it doesn't mean that CUDA is the best tool for every job.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"I think it's worth spending a little time talking about this problem in slightly more depth.","category":"page"},{"location":"content/intro/#The-Fragmentation-of-Modern-GPU-Interfaces","page":"Introduction","title":"The Fragmentation of Modern GPU Interfaces","text":"","category":"section"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"When you buy a CPU, it doesn't matter whether you buy one from AMD or Intel, both will work approximately the same regardless of whether you are using Python, C, Rust, or any other language. Unfortunately, that is not the case when it comes to GPUs. Everything is a complete mess and it is really hard to navigate for new programmers. In this section, I am going to do my best to explain that mess without also overwhelming you with too much information. Please bear with me.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"As stated before, CUDA only really works on NVIDIA devices. ROCm is probably the closest to CUDA you can get with AMD cards. If you are running a modern Mac (with Apple Silicon), then you will be encouraged to use Metal, which is a hybrid graphics and compute interface.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"None of these languages talk to each other. You can't run Metal on NVIDIA cards. You can't run ROCm on Macs.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"But what if you are writing \"real\" software and have multiple users, all with different hardware? One might use a Mac. Another might use an Intel GPU. Another, an NVIDIA one. What do you do?","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Good question. Really good question. Let me know when you have an answer because I am interested too.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"The way I see it, there are 2 solutions:","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Support all the different backends for each individual use-case.\nWrite your code in a cross-platform interface.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"I think option 1 is self-explanatory. You'll have to maintain some CUDA code for NVIDIA users. Some Metal code for Mac users. Some ROCm code for AMD Users. And so on.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Basically, any time you need to change one of your GPU functions, you need copy that change along to all the other vendors to keep all your users happy. It's a pain, but doable. It just requires a bit of testing and a few afternoons of debugging for each backend.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"But there must be a better way, right?","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Kinda. Cross-platform GPU interfaces allow you to write functions that run at essentially the same speed as vendor-specific APIs (like CUDA), but those functions are not limited to specific hardware, so the same code can run on AMD, Intel, Apple Silicon, and NVIDIA hardware. In fact, many cross-platform interfaces allow for that same code to run in parallel on the CPU as well. The Open Compute Language (OpenCL) can even run on many cell phones and Field Programmable Gate Arrays (FPGAs), which are separate devices used in completely different types of problems for performance reasons.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"So what's the catch?  Well, it's hard to overstate how incredibly dominant CUDA has been in the GPGPU space for so many years. Sure, you could write your code in a cross-platform way, but why would you? You would be taking a small performance hit (something like 10%) and it would take an extra week to write your code. Plus, all of the common GPU programming guides are in CUDA. Time is money, and it takes time to learn. From a business perspective, it's better to just pay an extra hundred dollars on an NVIDIA card and save yourself (and your employees) the hassle.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"To reiterate, almost all non-CUDA interfaces have the same drawback: they are not CUDA. This means that there is less documentation available. The code will be buggier and with less developer support. The experience simply won't be as smooth as CUDA. In a world where everyone is trying to get the absolute best performance possible as quickly as possible, these are huge issues.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Long story short, it's impossible to talk about GPU computing without acknowledging the big green elephant in the room: CUDA.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Still, I have no idea who is reading this book or what devices they have available. I can't count on everyone having an NVIDIA GPU to use, and for that reason alone I do not feel that CUDA is suitable for this work. That said, I am certain everyone will have some device at their disposal that can run GPU code, so I will focus on languages (or rather a single language) that I am confident the majority of my audience can use.","category":"page"},{"location":"content/intro/#If-not-CUDA,-then-What?","page":"Introduction","title":"If not CUDA, then What?","text":"","category":"section"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"After a lot of thought, I settled on using Julia and the KernelAbstractions(.jl) package for this book. There are benefits and drawbacks of this choice, which I could ramble about for hours, but in short, Julia provides:","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"A flexible software ecosystem that works on any GPU vendor (AMD, NVIDIA, Apple Silicon, Intel).\nThe ability to write code that can execute both on the GPU and in parallel on the CPU at the same time.\nA way to execute GPU code without writing GPU-specific functions or \"kernels.\"\nA straightforward package management approach so users don't have to think about library installation.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"There are a few other benefits, but this specific combination of useful features cannot be found anywhere else.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"To be clear, the Open Compute Language (OpenCL) also shares many of these advantages and even has a few distinct benefits over Julia as well. Unfortunately, OpenCL is a little less straightforward to use. The way I see it, this book is about teaching GPU concepts, and the JuliaGPU ecosystem lets me quickly start doing just that. If I were to write this book with OpenCL (or even CUDA), I would need to spend a significant amount of time explaining syntax and odd quirks to C (or god-forbid C++), that I just don't want to deal with. Again, I am actively encouraging you to rewrite this entire book in the language of your choice. For me, I'm planning to stick to Julia, but there is a core limitation to this choice I will mention at the end of this chapter.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Also, to be completely transparent, I have contributed to the GPU ecosystem in Julia in several ways, including the KernelAbstractions package we will be using for this work. This could be seen as a net benefit. After all, how often do you get to read a book from a developer of the API you will be using? On the other hand, I need to acknowledge my biases and let you (the reader) know that several of my opinions might be a little too favorable towards Julia and that your day-to-day experience with the language might fall a little short depending on your familiarity.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"On the other (other) hand, I really do try to be as objective as possible when talking about projects I am passionate about. There's nothing worse than being sold a tool you can't actually use in practice. That's why I am absolutely encouraging you to take the code in this book and rewrite it into the language of your choice.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"Alright. That's enough rambling. We'll be doing a lot more of it later. Now, let's talk the ...","category":"page"},{"location":"content/intro/#General-Structure-and-Limitations-of-this-Book","page":"Introduction","title":"General Structure and Limitations of this Book","text":"","category":"section"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"As much as I hate to say it, our time on this Earth is limited. It goes without saying that there are things I can cover, and things I can't.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"My ultimate goal with this book is to provide a \"quick-start\" guide for those wanting to learn how to get started with GPU computing. That means that I intend to cover a number of core ideas and concepts that are necessary to consider when trying to achieve good performance on the GPU as well as key applications that I find interesting and useful for a diverse background of research fields.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"note: Reviewer Notice\nI'll be coming back to this section later with a full overview once the chapters are more-or-less finalized","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"For now, I also want to quickly discuss several core limitations of this book:","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"We will not be surveying different languages. This book is primarily intended to teach concepts over code. Once you master everything here, it should be relatively straightforward to translate it to whatever language you need for your final application. With that said, I will be highlighting languages and their differences as they become relevant in their respective sections.\nWe will not be discussing specialized hardware that certain vendors add to their GPUs. This means no discussion of (for example) hardware rasterization, raytracing (except in software), or tensor cores.\nWe will not be analyzing performance via NVIDIA-specific tooling like NSight compute. I simply don't think it is fair to have a chapter on performance analysis that only works for NVIDIA devices.","category":"page"},{"location":"content/intro/","page":"Introduction","title":"Introduction","text":"With all that said, I think it's time to finally start coding! In the next chapter, I'll be introducing several core abstractions programmers use when writing GPU code and getting you started in running that code on your hardware (whatever that might be).","category":"page"},{"location":"content/splatting/#The-Jank","page":"-","title":"The Jank","text":"","category":"section"},{"location":"content/splatting/","page":"-","title":"-","text":"Yeah. I'm going to come out and say it. No matter what language or interface you use for GPU programming in 2024, you will probably find yourself at least a little disappointed. They all feel a little rough, lacking the polish that programmers are used to nowadays. It is downright impossible to describe this problem fully. GPU functions look odd when compared to their CPU counterparts. Also, many common programming techniques simply don't work on the GPU. Way back.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"In general, a \"language\" is a method of communication between two (or more) individuals. A \"programming language\" is a method to communicate with a computer. Programming languages typically require a translation (compilation) step to transform the user-submitted code to something that the computer can understand.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"Nowadays, many languages will have multiple compilation steps, and will first lower the user code into a Lower-Level Intermediate Representation (LLIR) before then compiling down to machine code. The core advantage here is that the lowered code can then be compiled to different hardware. Simply put, the final set of instructions for AMD and Intel machines might be different, but the intermediate representation can be shared.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"Many languages (Julia, Rust, and even C sometimes) will compile down to the same intermediate representation known as LLVM (which stands for Lower-Level Virtual Machine). This means that as long as the conversion from Julia to LLVM is done well, it should be (roughly) the same speed as C.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"In a sense, GPU programming is not as straightforward. Until now, I have been careful not to call the GPU protocols \"languages,\" because they usually take regular languages (C, Python, Julia, Rust, etc) and extend the functionality to run on a GPU. For this reason, I have instead called them \"interfaces,\" and you will regularly see them called Application Programming Interfaces (APIs) when people talk about them in the \"real\" world. It is important to note that because the GPU interfaces target the GPU (and not the CPU), the all boil down to a different intermediate representation than for the CPU.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"That said, some of the GPU interfaces will still compile down to something like LLVM that has been modified for the GPU (NVPTX for CUDA, for example). Others compile down to another intermediate representation entirely. For example, OpenCL (the Open Compute Language) and Vulkan (a graphics interface) both compile down to something called SPIRV.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"Now, I hear what you are saying, \"That's great! We've got ourselves an open standard (SPIRV) that has unified both graphics and compute! Isn't that a core issue we already talked about in this chapter? Surely all the other interfaces will rally behind it, right?\"","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"Ok. Good question. It's impossible to answer without diving (at least a little bit) into the weeds.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"Simply put, reality is not that simple. The problem with SPIRV is that it's a bit too broad. Unlike LLVM, which is the same no matter what language is using it (Julia, Rust, C), SPIRV has two distinctly different implementations for graphics and compute. That is to say that the SPIRV implementation for OpenCL (a compute language) is not the same as the SPIRV implementation for Vulkan (a graphics interface).","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"This is honestly maddening! What this means is that you cannot use compute functions written in OpenCL in a graphics language like Vulkan even though they both use SPIRV! Though it is entirely possible to work on the LLVM level and create applications that work across multiple CPU languages, the same is not true for GPU languages – not only because not all compute languages boil down to SPIRV, but SPIRV is not always the right SPIRV for specific uses.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"This little rant has a valuable piece of information hidden just below the surface. The state of GPU computing in 2024 is largely unpolished, and while reading this book, you might find yourself frustrated. It might feel like the software is holding you back from unleashing your true potential. In some ways, it is. Some seemingly simple questions might lead you down complicated paths and suddenly, you have spent months worrying about subtle nuances in different compilation strategies that make you feel like your entire codebase is held together by unchewed bubble gum.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"When such problems arise, it's important to breathe and reframe your question. Sometimes it will take time. Sometimes, there is no solution, and you will have to shrug your shoulders and work on something else for a while. But in most cases, there will be a solution to your problem. You just might need to get a little creative.","category":"page"},{"location":"content/splatting/","page":"-","title":"-","text":"It is important to keep in mind that CPU languages have had years (decades) to figure out how to create fast, efficient CPU code. GPU languages, on the other hand, are relatively new and have yet to stabilize on a lower-level scheme that works across all languages. No matter who you ask, the GPU ecosystem (at large) is incredibly messy right now. I really hope that this book helps clarify some of that mess.","category":"page"},{"location":"content/about_me/#About-the-Author","page":"About the Author","title":"About the Author","text":"","category":"section"},{"location":"content/about_me/","page":"About the Author","title":"About the Author","text":"I am Dr. James Schloss. I received my PhD in 2019 from the Okinawa Institute of Science and Technology Graduate University (OIST). While there, I studied quantum systems by simulating them with my Graphics Processing Unit (GPU). After that, I have been working off and on in the Julia Lab at MIT (a very prominent research arm of the Julia programming language), where I worked on various scientific computing projects such as climate and molecular simulations. All of these projects have been done on the GPU. Though I would not consider myself to be a core developer of the JuliaGPU ecosystem, I have contributed to several packages, including KernelAbstractions(.jl), the GPU interface we are using for this work.","category":"page"},{"location":"content/about_me/","page":"About the Author","title":"About the Author","text":"I also run the (relatively) popular youtube channel and twitch stream Leios Labs, where we have been developing a book for uncommon algorithms known as the Arcane Algorithm Archive. Some of the chapters in this book were inspired by that work. I have also worked with Grant Sanderson (3Blue1Brown) on several projects, including the Summer of Math Exposition, where we encouraged thousands of online content creators to make more math content online.","category":"page"},{"location":"content/about_me/","page":"About the Author","title":"About the Author","text":"I have always said, \"Your research is only as good as your ability to communicate it.\" The way I see it, there are very few people who have a deep understanding of GPU technology who are also good at communicating that understanding to others.","category":"page"},{"location":"content/about_me/","page":"About the Author","title":"About the Author","text":"This book is my attempt to do both. I might not be the best programmer. I might not be the best communicator. But I am going to try my best to make this work as understandable and enjoyable as possible.","category":"page"},{"location":"content/about_me/#Prominent-Reviewers","page":"About the Author","title":"Prominent Reviewers","text":"","category":"section"},{"location":"content/about_me/","page":"About the Author","title":"About the Author","text":"Any academic work is only as good as the peers who read and critique it. For that reason, I intend to keep a list of \"Prominent Reviewers\" who don't mind putting their name in this section. This should be a list of academics or core community members who have either been asked to review the work due to their expertise or have contributed significantly in the beta reading phase.","category":"page"},{"location":"content/reviewers/#Reviewer-Guidelines","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"","category":"section"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"This book is currently in a \"beta reading\" state. That means that I am actively looking for feedback from you, the beta reader. If there is anything funky about the book, please let me know on github or discord. Possible funkiness includes:","category":"page"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"Typos\nUnclear concepts\nUnnecessary rambling\nImpossible problems","category":"page"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"... And the like.","category":"page"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"note: Reviewer Notice\nI will also have a few reviewer notices scattered throughout the book for specific issues that I want more input on.","category":"page"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"Please keep in mind that I am trying to make GPU computing as easy to understand as possible. It's really hard to do this right. For example, I might over-explain a few concepts that otherwise might be seen as \"trivial\". I might also omit \"important\" jargon that I feel is unnecessary. I am really (overly) relying on you all to gently nudge the book in the right direction and welcome feedback from programmers of all levels.","category":"page"},{"location":"content/reviewers/#Prominent-Reviewers","page":"Reviewer Guidelines","title":"Prominent Reviewers","text":"","category":"section"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"I feel peer review is an essential part of any academic work. I also feel that it is important to be as transparent as possible about that review process.","category":"page"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"When the book is finalized, I will ask a few experts that I know to review the work and tell me what they think. With their permission, I will then put their name somewhere in the acknowledgments for the book (probably next to an \"About the Author\" section or something). If you would like to be considered as a \"Prominent Reviewer,\" please...","category":"page"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"Review a full chapter as if it were an academic paper. I mean, really rip the book to shreds and tell me how big of an idiot I am.\nLet me know your academic background so I can include you.","category":"page"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"I'll also be keeping an eye out for people who contribute substantially during the review process and reaching out to them to see if they would like to be listed as a \"Prominent Reviewer\" as well.","category":"page"},{"location":"content/reviewers/","page":"Reviewer Guidelines","title":"Reviewer Guidelines","text":"I think that's all for now. Thanks for reading the book and I hope it is helpful in some way!","category":"page"},{"location":"#What's-going-on?","page":"Welcome","title":"What's going on?","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"When I started my PhD in 2014, it was fairly uncommon for programmers to use their Graphics Processing Unit (GPU) for any computation beyond what was necessary for gaming or some graphical applications. The world has changed since then. Nowadays, it feels like the GPU is the most important piece of hardware on any computational device (supercomputers, desktops, phones, etc). Computer Generated Imagery (CGI) for movies and games have become almost photorealistic, and all of the necessary computation happens on the GPU. The fastest supercomputers in the world run GPUs. Machine learning models are trained using GPUs. Everyone needs the GPU for something.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"At the same time, there are very few good learning resources available to teach beginner programmers how to properly use their hardware. So that's what this book is. A gentle introduction to most of the programming concepts necessary to understand GPU computing through meaningful real-world applications.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"All of the content on this site was created by James Schloss (Leios). The code is released under the MIT license, which means you can use it for virtually any purpose as long as you attribute me and you let everyone know that my code is freely available under the MIT license for use. The text is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC-BY-NC-SA). This means that you are free to read, copy, modify, and remix any of the text here with a few key restrictions. Namely, the new (derivative) work:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Cannot make money.\nMust attribute me (James Schloss or Leios).\nMust be available for free with the same Creative Commons license. In other words, derivatives of derivatives of this work cannot make money and also must attribute me (and any additional authors).","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"if this book is useful for you, please consider purchasing it when it is officially released. Otherwise, please let me know if there are any typos, errors, or jank that you find along the way.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Thanks for reading and I hope the book helps you in some way!","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Welcome to the world of General Purpose Graphics Processing Unit (GPGPU) computation.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Dr. James Schloss (Leios)","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"(Image: CC BY NC SA)","category":"page"}]
}
